{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a pretty straight forward implementation of character level \n",
    "# text generator model. I this imlementation I am going to observe the\n",
    "# memory feature of Recurrent Neural Network with GRU Cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('text_data/science.txt', 'r') as f:\n",
    "    text= f.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary is a collection all the possible character that the text has for examples alphabets, numerics, punctuations etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab= set(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char2id is a dictionary to map each characters to a unique numeric Id whcih could be fed to the model, and id2char is a dictionary to map a numeric Id to a unique character which is useful to generate characters form the model predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2id= dict((c,i) for i,c in enumerate(vocab))\n",
    "id2char= dict((i,c) for i,c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function to convert list of given characters to Ids.\n",
    "# function excepts the list of characters and returns the list\n",
    "# of ids and the character which is not available is the vocabulary\n",
    "# has a unique id 0.\n",
    "def id_char(char):\n",
    "    vec= np.zeros(shape= (len(char),1), dtype= float)\n",
    "    for i,c in enumerate(char):\n",
    "        try:\n",
    "            j= char2id[c]\n",
    "            vec[i]= j\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return vec\n",
    "\n",
    "# one hot enoding the list of characters\n",
    "def vectorize_char(char):\n",
    "    vec= np.zeros(shape= (len(char),len(vocab)), dtype= float)\n",
    "    for i,c in enumerate(char):\n",
    "        try:\n",
    "            j= char2id[c]\n",
    "            vec[i,j]= 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return vec\n",
    "\n",
    "# this function accepts the list of one hot vectors and returns\n",
    "# the list of crresponding characters. <UKN> is a token for the\n",
    "# characters not available in the vocabulary\n",
    "def vector_characterize(vecs):\n",
    "    chars= ['<UKN>']*vecs.shape[0]\n",
    "    for i,j in enumerate(np.where(vecs==1)[1]):\n",
    "        try:\n",
    "            chars[i]= id2char[j]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model has 2 hidden layers each with 256 GRU Cells\n",
    "\n",
    "# The Rnn is truncated at every 100th time step i.e. the network\n",
    "# has the memory of last 100 time steps and predicts the next character\n",
    "# on the basis of that.\n",
    "\n",
    "# the id corresponding to each character is fed to the network\n",
    "# which generates the one hot encoded vector as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden= 256\n",
    "batch_size= 128\n",
    "time_step= 100\n",
    "input_dim= 1\n",
    "output_dim= len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= tf.placeholder(shape= [None,time_step,input_dim], dtype= tf.float32)\n",
    "Y= tf.placeholder(shape= [None,output_dim], dtype= tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_in= tf.Variable(tf.truncated_normal(shape= [input_dim,n_hidden], dtype= tf.float32, stddev= 0.001))\n",
    "b_in= tf.Variable(tf.constant(0.001, shape= [n_hidden]))\n",
    "w_h= tf.Variable(tf.truncated_normal(shape= [n_hidden,n_hidden], dtype= tf.float32, stddev= 0.001))\n",
    "b_h= tf.Variable(tf.constant(0.001, shape= [n_hidden]))\n",
    "w_out= tf.Variable(tf.truncated_normal(shape= [n_hidden,output_dim], dtype= tf.float32, stddev= 0.001))\n",
    "b_out= tf.Variable(tf.constant(0.001, shape= [output_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_in= tf.matmul(tf.reshape(X,(-1,input_dim)),w_in)+b_in\n",
    "X_in= tf.reshape(X_in, (-1,time_step,n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cells= tf.contrib.rnn.GRUCell(n_hidden)\n",
    "multilayer_cell= tf.contrib.rnn.MultiRNNCell([cells,cells], state_is_tuple= True)\n",
    "init_state= multilayer_cell.zero_state(batch_size, dtype= tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output,state= tf.nn.dynamic_rnn(multilayer_cell,X_in, initial_state= init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output= tf.unstack(tf.transpose(output, (1,0,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_= tf.matmul(output[-1],w_out)+b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= y_, labels= Y))\n",
    "optimize= tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init= tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolving straings of length 101 is fed to the list named data\n",
    "# where first 100 characters will be treated as the input and \n",
    "# last charater will be treated as output label.\n",
    "\n",
    "data= []\n",
    "for i in range(1000000):\n",
    "    data.append(text[i:i+101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model for 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    total_loss= 0\n",
    "    for j in range(5000):\n",
    "        x_batch= [id_char(data[j][:-1]) for j in \n",
    "                  range(batch_size*j,batch_size*(j+1))]\n",
    "        x_batch= np.array(x_batch)/len(vocab)\n",
    "        y_batch= [vectorize_char(data[j][-1])[0] for j in \n",
    "                  range(batch_size*j,batch_size*(j+1))]\n",
    "        \n",
    "        cost, _= sess.run([loss,optimize], feed_dict= {X: x_batch,\n",
    "                                                       Y: y_batch})\n",
    "        total_loss+= cost\n",
    "        print (i,j,cost)\n",
    "        \n",
    "    print (i,total_loss/5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in_text is a seed text fed to the model on the basis of which the model generates next 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the basis of scientific evidence, this discovery can be an invested the study says says the study in the mass of the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein the same traditions in the same protein \n"
     ]
    }
   ],
   "source": [
    "in_text= 'on the basis of scientific evidence, this discovery can '\n",
    "for i in range(500):\n",
    "    blank= ' '*(time_step-len(in_text))\n",
    "    in_= blank+in_text\n",
    "    if len(in_)>100:\n",
    "        in_= in_[len(in_)-100:]\n",
    "    x_batch= np.zeros(shape= (batch_size,time_step,input_dim))\n",
    "\n",
    "    x_batch[0]= id_char(in_)/len(vocab)\n",
    "    y_pred= sess.run(y_, feed_dict= {X: x_batch.reshape(-1,time_step,input_dim)})\n",
    "    index= sess.run(tf.argmax(y_pred[0],0))\n",
    "    char= id2char[index]\n",
    "    in_text+= char\n",
    "print (in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that the combination of all the characters predicted by the\n",
    "# model forms actual english words. Although sentences doesn't makes \n",
    "# sense but training the network for loger time and optimizing the\n",
    "# hyperparameters definetly will improve the model.\n",
    "\n",
    "# Another importent thing that can be noted from the prediction is\n",
    "# that the theme of the predicted text is rougly scientific which\n",
    "# is same as the theme of the training data, thus we could use this \n",
    "# model to generate text of specific theme and styles. For example\n",
    "# Shakespere style text can be generated using this model simply by\n",
    "# trainng the model over the corpus of Shakespere's writtings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
